{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Logistic Regression and different pre-process techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook the aim is to try different pre-process techniques and NLP models with Logistic regression classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\eric_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\eric_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score, confusion_matrix\n",
    "import spacy # (object oriented)\n",
    "import nltk # natural language tool kit (string oriented)\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#uncomment and Run this lines only once\n",
    "#!python -m spacy download en_core_web_lg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/data_usampl_60_40_cleaned.csv') #nrows only to get the fist 500 rows in the data for speed up testing\n",
    "#please remove nrows parameter if you want to try with the complete data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = data[['raw', 'toxic']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trudeau with a brain?  I assume you are taking...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Jones Act was immediately lifted to help T...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As long as the Church keeps preventing the Lor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Climate change, in the sense discussed in the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fake news...now she is lying. figures....she i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 raw  toxic\n",
       "0  Trudeau with a brain?  I assume you are taking...      1\n",
       "1  The Jones Act was immediately lifted to help T...      1\n",
       "2  As long as the Church keeps preventing the Lor...      0\n",
       "3  Climate change, in the sense discussed in the ...      0\n",
       "4  Fake news...now she is lying. figures....she i...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data in train and test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train['raw'], df_train['toxic'], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to record different models performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dataframe that will include the results\n",
    "results_table = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train,y_train,X_test,y_test,results_df,model_name=\"\", parameters='', comments=''):\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    predict_probab = model.predict_proba(X_test)[:,1]\n",
    "    duration = time.time() - start_time\n",
    "    duration_format = f\"{int(duration // 60)} minutes and {round(duration % 60, 2)} seconds\"\n",
    "\n",
    "    # Calculating all metrics\n",
    "\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "    roc_auc = roc_auc_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    conf_matrix = str(confusion_matrix(y_test, predictions))\n",
    "\n",
    "    # Create a dictionary including the results\n",
    "    results = {\n",
    "        'Name': model_name if model_name else model.__class__.__name__,\n",
    "        'Parameters': parameters,\n",
    "        'F1-Score': f1,\n",
    "        'AUC-ROC': roc_auc,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'Accuracy': accuracy,\n",
    "        'Confusion Matrix': conf_matrix,\n",
    "        'Training Time': duration_format,\n",
    "        'Comments': comments\n",
    "    }\n",
    "\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    new_row_df = pd.DataFrame([results])\n",
    "    # don't forget to append the result to the results dataframe\n",
    "    results_df = pd.concat([results_df, new_row_df], ignore_index=True)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the CountVectorizer to the training data\n",
    "vect = CountVectorizer().fit(X_train)\n",
    "\n",
    "# Prepare X_train for the function, transforming the different comments in the training data to a sparse matrix\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "# Prepare X_test for the function\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "\n",
    "# Initialize the model you want to try\n",
    "model = LogisticRegression(max_iter=1500)\n",
    "\n",
    "# Call the function and store the row in the variable result\n",
    "results_table = evaluate_model(model, X_train_vectorized, y_train, X_test_vectorized, y_test,results_table, parameters=\"\", comments=\"Baseline\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words(Binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the CountVectorizer to the training data\n",
    "vect = CountVectorizer(binary=True).fit(X_train)\n",
    "\n",
    "# Prepare X_train for the function, transforming the different comments in the training data to a sparse matrix\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "# Prepare X_test for the function\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "\n",
    "# Initialize the model you want to try\n",
    "model = LogisticRegression(max_iter=1500)\n",
    "\n",
    "# Call the function and store the row in the variable result\n",
    "results_table = evaluate_model(model, X_train_vectorized, y_train, X_test_vectorized, y_test,results_table, parameters=\"binary\", comments=\"Bag of words - Binary\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words (Binary + Stop Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))                       \n",
    "# stop_words contains a list of 179 words that we want to remove from our comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the CountVectorizer to the training data\n",
    "vect = CountVectorizer(binary=True, stop_words=list(stop_words)).fit(X_train)\n",
    "\n",
    "# Prepare X_train for the function, transforming the different comments in the training data to a sparse matrix\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "# Prepare X_test for the function\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "\n",
    "# Initialize the model you want to try\n",
    "model = LogisticRegression(max_iter=1500)\n",
    "\n",
    "# Call the function and store the row in the variable result\n",
    "results_table = evaluate_model(model, X_train_vectorized, y_train, X_test_vectorized, y_test,results_table, parameters=\"binary,stopwords\", comments=\"Bag of words - Binary/StopWords\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF - IDF + LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TfidfVectorizer with min_df\n",
    "tfidf_vect = TfidfVectorizer(min_df=30)\n",
    "\n",
    "# Prepare X_train for the function\n",
    "X_train_tfidf = tfidf_vect.fit_transform(X_train)\n",
    "\n",
    "# Prepare X_test for the function\n",
    "X_test_tfidf = tfidf_vect.transform(X_test)\n",
    "\n",
    "# Initialize the model you want to try\n",
    "model = LogisticRegression(max_iter=1500)\n",
    "\n",
    "# Call the function and store the row in the variable result\n",
    "results_table = evaluate_model(model, X_train_tfidf, y_train, X_test_tfidf, y_test,results_table, parameters=\"min_df=30\", comments=\"TfidfVectorizer\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming(Bag of words) + LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initializing stemmer and countvectorizer \n",
    "stemmer = nltk.PorterStemmer()\n",
    "cv_analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    ''' \n",
    "    In this function the text is first passed through the build_analyzer() and then each word in the text is stemmed to its base form\n",
    "    '''\n",
    "    return (stemmer.stem(w) for w in cv_analyzer(doc))\n",
    "\n",
    "# define CountVectorizer with stemming function \n",
    "stem_vectorizer = CountVectorizer(analyzer = stemmed_words)\n",
    "\n",
    "# Prepare X_train for the function\n",
    "X_train_stem_vectorized = stem_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Prepare X_test for the function\n",
    "X_test_stem_vectorized = stem_vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize the model you want to try\n",
    "model = LogisticRegression(max_iter=2500)\n",
    "\n",
    "# Call the function and store the row in the variable result\n",
    "results_table = evaluate_model(model, X_train_stem_vectorized, y_train, X_test_stem_vectorized, y_test, results_table, parameters=\"\", comments=\"Stemming_cv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming(Bag of words(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initializing stemmer and countvectorizer with Stop Words\n",
    "stemmer = nltk.PorterStemmer()\n",
    "cv_analyzer = CountVectorizer(stop_words=list(stop_words)).build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    ''' \n",
    "    In this function the text is first passed through the build_analyzer() and then each word in the text is stemmed to its base form\n",
    "    '''\n",
    "    return (stemmer.stem(w) for w in cv_analyzer(doc))\n",
    "\n",
    "# define CountVectorizer with stemming function \n",
    "stem_vectorizer = CountVectorizer(analyzer = stemmed_words)\n",
    "\n",
    "# Prepare X_train for the function\n",
    "X_train_stem_vectorized = stem_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Prepare X_test for the function\n",
    "X_test_stem_vectorized = stem_vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize the model you want to try\n",
    "model = LogisticRegression(max_iter=2500)\n",
    "\n",
    "# Call the function and store the row in the variable result\n",
    "results_table = evaluate_model(model, X_train_stem_vectorized, y_train, X_test_stem_vectorized, y_test,results_table, parameters=\"stopwords\", comments=\"Stemming_cv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming with TF - IDF and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "                         \n",
    "# stop_words contains a list of 179 words that we want to remove from our comments\n",
    "\n",
    "# Initializing stemmer and countvectorizer with Stop Words\n",
    "stemmer = nltk.PorterStemmer()\n",
    "tfidf_analyzer = TfidfVectorizer(min_df=30, stop_words=list(stop_words)).build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    ''' \n",
    "    In this function the text is first passed through the build_analyzer() and then each word in the text is stemmed to its base form\n",
    "    '''\n",
    "    return (stemmer.stem(w) for w in cv_analyzer(doc))\n",
    "\n",
    "# define CountVectorizer with stemming function \n",
    "stem_vectorizer = CountVectorizer(analyzer = stemmed_words)\n",
    "\n",
    "# Prepare X_train for the function\n",
    "X_train_stem_vectorized = stem_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Prepare X_test for the function\n",
    "X_test_stem_vectorized = stem_vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize the model you want to try\n",
    "model = LogisticRegression(max_iter=2500)\n",
    "\n",
    "# Call the function and store the row in the variable result\n",
    "results_table = evaluate_model(model, X_train_stem_vectorized, y_train, X_test_stem_vectorized, y_test,results_table, parameters=\"min_df=30, stopwords\", comments=\"Stemming_tfidf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization with Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "WNlemma = nltk.WordNetLemmatizer()\n",
    "cv_analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def lemmatize_word(doc):\n",
    "    ''' \n",
    "    In this function the text is first passed through the build_analyzer() and then each word in the text is stemmed to its base form\n",
    "    '''\n",
    "    return (WNlemma.lemmatize(t) for t in cv_analyzer(doc))\n",
    "\n",
    "# define CountVectorizer with Lemmatization function \n",
    "lemm_vectorizer = CountVectorizer(analyzer = lemmatize_word)\n",
    "\n",
    "# Prepare X_train for the function\n",
    "X_train_lemm_vectorized = lemm_vectorizer.fit_transform(X_train)\n",
    "# Prepare X_test for the function\n",
    "X_test_lemm_vectorized  = lemm_vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize the model you want to try\n",
    "model = LogisticRegression(max_iter=2500)\n",
    "\n",
    "# Call the function and store the row in the variable result\n",
    "results_table = evaluate_model(model, X_train_lemm_vectorized, y_train, X_test_lemm_vectorized, y_test,results_table, parameters=\"\", comments=\"lemmatization_cv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "WNlemma = nltk.WordNetLemmatizer()\n",
    "cv_analyzer = TfidfVectorizer(min_df=30).build_analyzer()\n",
    "\n",
    "def lemmatize_word(doc):\n",
    "    ''' \n",
    "    In this function the text is first passed through the build_analyzer() and then each word in the text is stemmed to its base form\n",
    "    '''\n",
    "    return (WNlemma.lemmatize(t) for t in cv_analyzer(doc))\n",
    "\n",
    "# define CountVectorizer with Lemmatization function \n",
    "lemm_vectorizer = CountVectorizer(analyzer = lemmatize_word)\n",
    "\n",
    "# Prepare X_train for the function\n",
    "X_train_lemm_vectorized = lemm_vectorizer.fit_transform(X_train)\n",
    "# Prepare X_test for the function\n",
    "X_test_lemm_vectorized  = lemm_vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize the model you want to try\n",
    "model = LogisticRegression(max_iter=2500)\n",
    "\n",
    "# Call the function and store the row in the variable result\n",
    "results_table = evaluate_model(model, X_train_lemm_vectorized, y_train, X_test_lemm_vectorized, y_test,results_table, parameters=\"min_df=30\", comments=\"lemmatization_tfidf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization with Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialization\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "WNlemma = nltk.WordNetLemmatizer()\n",
    "cv_analyzer = CountVectorizer(stop_words=list(stop_words)).build_analyzer()\n",
    "\n",
    "def lemmatize_word(doc):\n",
    "    ''' \n",
    "    In this function the text is first passed through the build_analyzer() and then each word in the text is stemmed to its base form\n",
    "    '''\n",
    "    return (WNlemma.lemmatize(t) for t in cv_analyzer(doc))\n",
    "\n",
    "# define CountVectorizer with Lemmatization function \n",
    "lemm_vectorizer = CountVectorizer(analyzer = lemmatize_word)\n",
    "\n",
    "# Prepare X_train for the function\n",
    "X_train_lemm_vectorized = lemm_vectorizer.fit_transform(X_train)\n",
    "# Prepare X_test for the function\n",
    "X_test_lemm_vectorized  = lemm_vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize the model you want to try\n",
    "model = LogisticRegression(max_iter=2500)\n",
    "\n",
    "# Call the function and store the row in the variable result\n",
    "results_table = evaluate_model(model, X_train_lemm_vectorized, y_train, X_test_lemm_vectorized, y_test,results_table, parameters=\"stopwords\", comments=\"lemmatization_cv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors - Spacy library - Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This initialize a pre-trained model (the large version) that uses Neural Networks to build word vectors\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# convert words into vectors and Prepare X_train for the function\n",
    "docs = [nlp(text) for text in X_train]\n",
    "X_train_word_vectors = [x.vector for x in docs]\n",
    "\n",
    "# Prepare X_test for the function\n",
    "docs_test = [nlp(text) for text in X_test]\n",
    "X_test_word_vectors = [x.vector for x in docs_test]\n",
    "\n",
    "# Initialize the model you want to try\n",
    "model = LogisticRegression(max_iter=2500)\n",
    "\n",
    "# Call the function and store the row in the variable result\n",
    "results_table = evaluate_model(model, X_train_word_vectors, y_train, X_test_word_vectors, y_test,results_table, parameters=\"\", comments=\"word_vectors_spacy_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>AUC-ROC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td></td>\n",
       "      <td>0.463768</td>\n",
       "      <td>0.625071</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.372093</td>\n",
       "      <td>0.704</td>\n",
       "      <td>[[72 10]\\n [27 16]]</td>\n",
       "      <td>0 minutes and 0.11 seconds</td>\n",
       "      <td>Baseline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>binary</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.618973</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.372093</td>\n",
       "      <td>0.696</td>\n",
       "      <td>[[71 11]\\n [27 16]]</td>\n",
       "      <td>0 minutes and 0.06 seconds</td>\n",
       "      <td>Bag of words - Binary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>binary,stopwords</td>\n",
       "      <td>0.474576</td>\n",
       "      <td>0.650596</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.325581</td>\n",
       "      <td>0.752</td>\n",
       "      <td>[[80  2]\\n [29 14]]</td>\n",
       "      <td>0 minutes and 0.03 seconds</td>\n",
       "      <td>Bag of words - Binary/StopWords</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>min_df=30</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.527085</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.648</td>\n",
       "      <td>[[75  7]\\n [37  6]]</td>\n",
       "      <td>0 minutes and 0.01 seconds</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td></td>\n",
       "      <td>0.465753</td>\n",
       "      <td>0.618406</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.395349</td>\n",
       "      <td>0.688</td>\n",
       "      <td>[[69 13]\\n [26 17]]</td>\n",
       "      <td>0 minutes and 0.08 seconds</td>\n",
       "      <td>Stemming_cv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>stopwords</td>\n",
       "      <td>0.477612</td>\n",
       "      <td>0.637266</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.372093</td>\n",
       "      <td>0.720</td>\n",
       "      <td>[[74  8]\\n [27 16]]</td>\n",
       "      <td>0 minutes and 0.12 seconds</td>\n",
       "      <td>Stemming_cv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>min_df=30, stopwords</td>\n",
       "      <td>0.477612</td>\n",
       "      <td>0.637266</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.372093</td>\n",
       "      <td>0.720</td>\n",
       "      <td>[[74  8]\\n [27 16]]</td>\n",
       "      <td>0 minutes and 0.04 seconds</td>\n",
       "      <td>Stemming_tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td></td>\n",
       "      <td>0.478873</td>\n",
       "      <td>0.630601</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.395349</td>\n",
       "      <td>0.704</td>\n",
       "      <td>[[71 11]\\n [26 17]]</td>\n",
       "      <td>0 minutes and 0.07 seconds</td>\n",
       "      <td>lemmatization_cv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>min_df=30</td>\n",
       "      <td>0.478873</td>\n",
       "      <td>0.630601</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.395349</td>\n",
       "      <td>0.704</td>\n",
       "      <td>[[71 11]\\n [26 17]]</td>\n",
       "      <td>0 minutes and 0.06 seconds</td>\n",
       "      <td>lemmatization_tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>stopwords</td>\n",
       "      <td>0.412698</td>\n",
       "      <td>0.608480</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.302326</td>\n",
       "      <td>0.704</td>\n",
       "      <td>[[75  7]\\n [30 13]]</td>\n",
       "      <td>0 minutes and 0.03 seconds</td>\n",
       "      <td>lemmatization_cv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td></td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.668179</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.604651</td>\n",
       "      <td>0.688</td>\n",
       "      <td>[[60 22]\\n [17 26]]</td>\n",
       "      <td>0 minutes and 0.12 seconds</td>\n",
       "      <td>word_vectors_spacy_lg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Name            Parameters  F1-Score   AUC-ROC  Precision  \\\n",
       "0   LogisticRegression                        0.463768  0.625071   0.615385   \n",
       "1   LogisticRegression                binary  0.457143  0.618973   0.592593   \n",
       "2   LogisticRegression      binary,stopwords  0.474576  0.650596   0.875000   \n",
       "3   LogisticRegression             min_df=30  0.214286  0.527085   0.461538   \n",
       "4   LogisticRegression                        0.465753  0.618406   0.566667   \n",
       "5   LogisticRegression             stopwords  0.477612  0.637266   0.666667   \n",
       "6   LogisticRegression  min_df=30, stopwords  0.477612  0.637266   0.666667   \n",
       "7   LogisticRegression                        0.478873  0.630601   0.607143   \n",
       "8   LogisticRegression             min_df=30  0.478873  0.630601   0.607143   \n",
       "9   LogisticRegression             stopwords  0.412698  0.608480   0.650000   \n",
       "10  LogisticRegression                        0.571429  0.668179   0.541667   \n",
       "\n",
       "      Recall  Accuracy     Confusion Matrix               Training Time  \\\n",
       "0   0.372093     0.704  [[72 10]\\n [27 16]]  0 minutes and 0.11 seconds   \n",
       "1   0.372093     0.696  [[71 11]\\n [27 16]]  0 minutes and 0.06 seconds   \n",
       "2   0.325581     0.752  [[80  2]\\n [29 14]]  0 minutes and 0.03 seconds   \n",
       "3   0.139535     0.648  [[75  7]\\n [37  6]]  0 minutes and 0.01 seconds   \n",
       "4   0.395349     0.688  [[69 13]\\n [26 17]]  0 minutes and 0.08 seconds   \n",
       "5   0.372093     0.720  [[74  8]\\n [27 16]]  0 minutes and 0.12 seconds   \n",
       "6   0.372093     0.720  [[74  8]\\n [27 16]]  0 minutes and 0.04 seconds   \n",
       "7   0.395349     0.704  [[71 11]\\n [26 17]]  0 minutes and 0.07 seconds   \n",
       "8   0.395349     0.704  [[71 11]\\n [26 17]]  0 minutes and 0.06 seconds   \n",
       "9   0.302326     0.704  [[75  7]\\n [30 13]]  0 minutes and 0.03 seconds   \n",
       "10  0.604651     0.688  [[60 22]\\n [17 26]]  0 minutes and 0.12 seconds   \n",
       "\n",
       "                           Comments  \n",
       "0                          Baseline  \n",
       "1             Bag of words - Binary  \n",
       "2   Bag of words - Binary/StopWords  \n",
       "3                   TfidfVectorizer  \n",
       "4                       Stemming_cv  \n",
       "5                       Stemming_cv  \n",
       "6                    Stemming_tfidf  \n",
       "7                  lemmatization_cv  \n",
       "8               lemmatization_tfidf  \n",
       "9                  lemmatization_cv  \n",
       "10            word_vectors_spacy_lg  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
