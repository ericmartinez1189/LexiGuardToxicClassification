{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ChnD36qHhuy"
   },
   "source": [
    "# Modeling Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLTUtsD6HupB"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nisbn3mTG0fj",
    "outputId": "c3a65a47-0bea-4de2-c594-1b5ca57dec48"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import time; full_run_time_start = time.time() # start timing exec right away\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from scipy import sparse\n",
    "import re\n",
    "import os\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, classification_report, f1_score,\\\n",
    "    accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# display all df columns (default is 20)\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions for testing models and tracking results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty df for storing results\n",
    "test_results = pd.DataFrame(columns=['model_name',\n",
    "                                'model_params',\n",
    "                                'data_desc',\n",
    "                                'data_size',\n",
    "                                'features_no',\n",
    "                                'f1',\n",
    "                                'acc',\n",
    "                                'recall',\n",
    "                                'prec',\n",
    "                                'roc_auc',\n",
    "                                'cf_matrix',\n",
    "                                'train_time',\n",
    "                                'notes'])\n",
    "\n",
    "def test_model(model, model_name, model_params, data_desc, X, y, notes=''):\n",
    "    '''\n",
    "    test_model(model, model_params, data_desc, X, y, notes='')\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model: instance of model to test\n",
    "    model_name: name of model\n",
    "    model_params: dict of (hyper)parameters passed to model\n",
    "    data_desc: description of dataset (preprocessing steps etc.)\n",
    "    X: feature array \n",
    "    y: target/label array\n",
    "    notes: additional notes (default: empty string)\n",
    "    '''\n",
    "\n",
    "    # Split data using default of 75% for train, 25% for test.\n",
    "    # Make sure test data has same toxic/nontoxic ratio as train data by\n",
    "    # using stratify parameter.\n",
    "    X_train, X_test, y_train, y_test =\\\n",
    "        train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "    \n",
    "    # train model and time execution\n",
    "    train_time_start = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - train_time_start\n",
    "    train_time_str = f'{int(train_time // 60)}m {round(train_time % 60)}s'\n",
    "\n",
    "    # Make predictions on test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "    return {'model_name': model_name,\n",
    "            'model_params': model_params,\n",
    "            'data_desc': data_desc,\n",
    "            'data_size': X.shape[0],\n",
    "            'features_no': X.shape[1],\n",
    "            'f1': round(f1_score(y_test, y_pred), 5),\n",
    "            'acc': round(accuracy_score(y_test, y_pred), 5),\n",
    "            'recall': round(recall_score(y_test, y_pred), 5),\n",
    "            'prec': round(precision_score(y_test, y_pred), 5),\n",
    "            'roc_auc': round(roc_auc_score(y_test, y_pred_proba), 5),\n",
    "            'cf_matrix': confusion_matrix(y_test, y_pred),\n",
    "            'train_time': train_time_str,\n",
    "            'notes': notes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_test_result(result):\n",
    "    test_results.loc[len(test_results)] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MP847vfIJMN"
   },
   "source": [
    "## Load data (final data file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "r6YNY0NIIL4d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(398434, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/data_usampl_60_40_cleaned.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for NaN's ...\n",
      "raw                    0\n",
      "clean                  0\n",
      "clean_pp               0\n",
      "clean_pp_lemma         0\n",
      "clean_pp_lemma_stop    0\n",
      "toxic                  0\n",
      "dtype: int64\n",
      "\n",
      "Rows before dropping: 398434\n",
      "Rows after: 398434\n",
      "Rows dropped: 0\n"
     ]
    }
   ],
   "source": [
    "print('Checking for NaN\\'s ...')\n",
    "print(df.isna().sum())\n",
    "rows_before = df.shape[0]\n",
    "print(\"\\nRows before dropping:\", rows_before)\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "rows_after = df.shape[0]\n",
    "print('Rows after:', rows_after)\n",
    "print('Rows dropped:', rows_before - rows_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "lZffM2npRCPf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw</th>\n",
       "      <th>clean</th>\n",
       "      <th>clean_pp</th>\n",
       "      <th>clean_pp_lemma</th>\n",
       "      <th>clean_pp_lemma_stop</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trudeau with a brain?  I assume you are taking...</td>\n",
       "      <td>Trudeau with a brain? I assume you are taking ...</td>\n",
       "      <td>trudeau with a brain i assume you are taking a...</td>\n",
       "      <td>trudeau with a brain i assume you be take abou...</td>\n",
       "      <td>trudeau brain assume take pierre imagine</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Jones Act was immediately lifted to help T...</td>\n",
       "      <td>The Jones Act was immediately lifted to help T...</td>\n",
       "      <td>the jones act was immediately lifted to help t...</td>\n",
       "      <td>the jones act be immediately lift to help texa...</td>\n",
       "      <td>jones act immediately lift help texas florida ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As long as the Church keeps preventing the Lor...</td>\n",
       "      <td>As long as the Church keeps preventing the Lor...</td>\n",
       "      <td>as long as the church keeps preventing the lor...</td>\n",
       "      <td>as long as the church keep prevent the lord fr...</td>\n",
       "      <td>long church keep prevent lord call woman sacra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Climate change, in the sense discussed in the ...</td>\n",
       "      <td>Climate change, in the sense discussed in the ...</td>\n",
       "      <td>climate change in the sense discussed in the p...</td>\n",
       "      <td>climate change in the sense discuss in the pop...</td>\n",
       "      <td>climate change sense discuss pope encyclical d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fake news...now she is lying. figures....she i...</td>\n",
       "      <td>Fake news...now she is lying. figures....she i...</td>\n",
       "      <td>fake news now she is lying figures she is maki...</td>\n",
       "      <td>fake news now she be lie figure she be make he...</td>\n",
       "      <td>fake news lie figure make million gosh darn de...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 raw  \\\n",
       "0  Trudeau with a brain?  I assume you are taking...   \n",
       "1  The Jones Act was immediately lifted to help T...   \n",
       "2  As long as the Church keeps preventing the Lor...   \n",
       "3  Climate change, in the sense discussed in the ...   \n",
       "4  Fake news...now she is lying. figures....she i...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  Trudeau with a brain? I assume you are taking ...   \n",
       "1  The Jones Act was immediately lifted to help T...   \n",
       "2  As long as the Church keeps preventing the Lor...   \n",
       "3  Climate change, in the sense discussed in the ...   \n",
       "4  Fake news...now she is lying. figures....she i...   \n",
       "\n",
       "                                            clean_pp  \\\n",
       "0  trudeau with a brain i assume you are taking a...   \n",
       "1  the jones act was immediately lifted to help t...   \n",
       "2  as long as the church keeps preventing the lor...   \n",
       "3  climate change in the sense discussed in the p...   \n",
       "4  fake news now she is lying figures she is maki...   \n",
       "\n",
       "                                      clean_pp_lemma  \\\n",
       "0  trudeau with a brain i assume you be take abou...   \n",
       "1  the jones act be immediately lift to help texa...   \n",
       "2  as long as the church keep prevent the lord fr...   \n",
       "3  climate change in the sense discuss in the pop...   \n",
       "4  fake news now she be lie figure she be make he...   \n",
       "\n",
       "                                 clean_pp_lemma_stop  toxic  \n",
       "0           trudeau brain assume take pierre imagine      1  \n",
       "1  jones act immediately lift help texas florida ...      1  \n",
       "2  long church keep prevent lord call woman sacra...      0  \n",
       "3  climate change sense discuss pope encyclical d...      0  \n",
       "4  fake news lie figure make million gosh darn de...      1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Create smaller sample from data to speed up experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using full data (398434 rows).\n"
     ]
    }
   ],
   "source": [
    "sample_size = None\n",
    "\n",
    "# uncomment to create sample of desired size\n",
    "#sample_size = 50_000\n",
    "\n",
    "if sample_size != None:\n",
    "    # ratio toxic/nontoxic\n",
    "    tox_perc = 0.4\n",
    "    nontox_perc = 0.6\n",
    "\n",
    "    # number of toxic/nontoxic rows\n",
    "    sample_size_tox = int(sample_size * tox_perc)\n",
    "    sample_size_nontox = int(sample_size * nontox_perc)\n",
    "\n",
    "    sample_tox = df[df['toxic'] == 1].sample(sample_size_tox,\n",
    "                                             random_state=42)\n",
    "    sample_nontox = df[df['toxic'] == 0].sample(sample_size_nontox,\n",
    "                                                random_state=42)\n",
    "\n",
    "    df = pd.concat([sample_tox, sample_nontox])\n",
    "    print(f'Using sample ({df.shape[0]} rows).')\n",
    "\n",
    "else:\n",
    "    print(f'Using full data ({df.shape[0]} rows).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create label/target variable and check for imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "29jE5PSrPFE2"
   },
   "outputs": [],
   "source": [
    "target = df['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nontoxic (0): 238652 (59.9 %)\n",
      "Toxic (1): 159782 (40.1 %)\n"
     ]
    }
   ],
   "source": [
    "value_counts = target.value_counts()\n",
    "nontoxic_count = value_counts[0]\n",
    "toxic_count = value_counts[1]\n",
    "nontoxic_perc =\\\n",
    "    round((nontoxic_count / (nontoxic_count + toxic_count)) * 100, 1)\n",
    "toxic_perc =\\\n",
    "    round((toxic_count / (nontoxic_count + toxic_count)) * 100, 1)\n",
    "\n",
    "print(f'Nontoxic (0): {nontoxic_count} ({nontoxic_perc} %)')\n",
    "print(f'Toxic (1): {toxic_count} ({toxic_perc} %)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow(data):\n",
    "    vect = CountVectorizer()\n",
    "    return vect.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run baseline model (logistic regression) on different data cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for model\n",
    "params = {'max_iter': 2_000}\n",
    "\n",
    "# load model with parameters\n",
    "lr = LogisticRegression(**params)\n",
    "\n",
    "test_result = test_model(lr, 'BASELINE (logistic regression)', params,\n",
    "                    'bag of words on col \"raw\"', bow(df['raw']), target)\n",
    "store_test_result(test_result)\n",
    "\n",
    "test_result = test_model(lr, 'BASELINE (logistic regression)', params,\n",
    "                    'bag of words on col \"clean\"', bow(df['clean']), target)\n",
    "store_test_result(test_result)\n",
    "\n",
    "test_result = test_model(lr, 'BASELINE (logistic regression)', params,\n",
    "                    'bag of words on col \"clean_pp\"', bow(df['clean_pp']), target)\n",
    "store_test_result(test_result)\n",
    "\n",
    "test_result = test_model(lr, 'BASELINE (logistic regression)', params,\n",
    "                    'bag of words on col \"clean_pp_lemma\"', bow(df['clean_pp_lemma']), target)\n",
    "store_test_result(test_result)\n",
    "\n",
    "test_result = test_model(lr, 'BASELINE (logistic regression)', params,\n",
    "                    'bag of words on col \"clean_pp_lemma_stop\"', bow(df['clean_pp_lemma_stop']), target)\n",
    "store_test_result(test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show test results + total exec time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>model_params</th>\n",
       "      <th>data_desc</th>\n",
       "      <th>data_size</th>\n",
       "      <th>features_no</th>\n",
       "      <th>f1</th>\n",
       "      <th>acc</th>\n",
       "      <th>recall</th>\n",
       "      <th>prec</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>cf_matrix</th>\n",
       "      <th>train_time</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BASELINE (logistic regression)</td>\n",
       "      <td>{'max_iter': 2000}</td>\n",
       "      <td>bag of words on col \"raw\"</td>\n",
       "      <td>398434</td>\n",
       "      <td>144000</td>\n",
       "      <td>0.82622</td>\n",
       "      <td>0.86685</td>\n",
       "      <td>0.78927</td>\n",
       "      <td>0.86679</td>\n",
       "      <td>0.92837</td>\n",
       "      <td>[[43855, 3876], [6734, 25222]]</td>\n",
       "      <td>3m 1s</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BASELINE (logistic regression)</td>\n",
       "      <td>{'max_iter': 2000}</td>\n",
       "      <td>bag of words on col \"clean\"</td>\n",
       "      <td>398434</td>\n",
       "      <td>129114</td>\n",
       "      <td>0.82617</td>\n",
       "      <td>0.86682</td>\n",
       "      <td>0.78921</td>\n",
       "      <td>0.86676</td>\n",
       "      <td>0.92869</td>\n",
       "      <td>[[43854, 3877], [6736, 25220]]</td>\n",
       "      <td>3m 20s</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BASELINE (logistic regression)</td>\n",
       "      <td>{'max_iter': 2000}</td>\n",
       "      <td>bag of words on col \"clean_pp\"</td>\n",
       "      <td>398434</td>\n",
       "      <td>128741</td>\n",
       "      <td>0.82622</td>\n",
       "      <td>0.86687</td>\n",
       "      <td>0.78921</td>\n",
       "      <td>0.86688</td>\n",
       "      <td>0.92867</td>\n",
       "      <td>[[43858, 3873], [6736, 25220]]</td>\n",
       "      <td>3m 20s</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BASELINE (logistic regression)</td>\n",
       "      <td>{'max_iter': 2000}</td>\n",
       "      <td>bag of words on col \"clean_pp_lemma\"</td>\n",
       "      <td>398434</td>\n",
       "      <td>115213</td>\n",
       "      <td>0.82544</td>\n",
       "      <td>0.86648</td>\n",
       "      <td>0.78724</td>\n",
       "      <td>0.86754</td>\n",
       "      <td>0.92954</td>\n",
       "      <td>[[43890, 3841], [6799, 25157]]</td>\n",
       "      <td>3m 29s</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BASELINE (logistic regression)</td>\n",
       "      <td>{'max_iter': 2000}</td>\n",
       "      <td>bag of words on col \"clean_pp_lemma_stop\"</td>\n",
       "      <td>398434</td>\n",
       "      <td>115174</td>\n",
       "      <td>0.82363</td>\n",
       "      <td>0.86521</td>\n",
       "      <td>0.78480</td>\n",
       "      <td>0.86650</td>\n",
       "      <td>0.92815</td>\n",
       "      <td>[[43867, 3864], [6877, 25079]]</td>\n",
       "      <td>0m 46s</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       model_name        model_params  \\\n",
       "0  BASELINE (logistic regression)  {'max_iter': 2000}   \n",
       "1  BASELINE (logistic regression)  {'max_iter': 2000}   \n",
       "2  BASELINE (logistic regression)  {'max_iter': 2000}   \n",
       "3  BASELINE (logistic regression)  {'max_iter': 2000}   \n",
       "4  BASELINE (logistic regression)  {'max_iter': 2000}   \n",
       "\n",
       "                                   data_desc  data_size  features_no       f1  \\\n",
       "0                  bag of words on col \"raw\"     398434       144000  0.82622   \n",
       "1                bag of words on col \"clean\"     398434       129114  0.82617   \n",
       "2             bag of words on col \"clean_pp\"     398434       128741  0.82622   \n",
       "3       bag of words on col \"clean_pp_lemma\"     398434       115213  0.82544   \n",
       "4  bag of words on col \"clean_pp_lemma_stop\"     398434       115174  0.82363   \n",
       "\n",
       "       acc   recall     prec  roc_auc                       cf_matrix  \\\n",
       "0  0.86685  0.78927  0.86679  0.92837  [[43855, 3876], [6734, 25222]]   \n",
       "1  0.86682  0.78921  0.86676  0.92869  [[43854, 3877], [6736, 25220]]   \n",
       "2  0.86687  0.78921  0.86688  0.92867  [[43858, 3873], [6736, 25220]]   \n",
       "3  0.86648  0.78724  0.86754  0.92954  [[43890, 3841], [6799, 25157]]   \n",
       "4  0.86521  0.78480  0.86650  0.92815  [[43867, 3864], [6877, 25079]]   \n",
       "\n",
       "  train_time notes  \n",
       "0      3m 1s        \n",
       "1     3m 20s        \n",
       "2     3m 20s        \n",
       "3     3m 29s        \n",
       "4     0m 46s        "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full run time: 18m 54s\n"
     ]
    }
   ],
   "source": [
    "full_run_time = time.time() - full_run_time_start\n",
    "print(f'Full run time: {int(full_run_time // 60)}m {round(full_run_time % 60)}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Result\n",
    "\n",
    "The best result achieved with a LogisticRegression algorithm was 0.53, this will serve as a Baseline comparison reference for further modeling attempts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate average comment length on cleaned data (before preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average comment length:\n",
      "287 characters\n",
      "50 words\n"
     ]
    }
   ],
   "source": [
    "# characters\n",
    "comm_len_chars = df['clean'].apply(lambda s: len(s))\n",
    "avg_comm_len_chars = comm_len_chars.sum() / len(comm_len_chars)\n",
    "\n",
    "# words (rough count)\n",
    "comm_len_words = df['clean']\\\n",
    "    .apply(lambda s: len(re.findall(r'\\S+', s)))\n",
    "avg_comm_len_words = comm_len_words.sum() / len(comm_len_words)\n",
    "\n",
    "print('Average comment length:')\n",
    "print(round(avg_comm_len_chars), 'characters')\n",
    "print(round(avg_comm_len_words), 'words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
